---
title: "LLM Hallucination Detection and Evaluation"
collection: projects
permalink: /projects/2024-01-01-llm-hallucination-detection
excerpt: 'Developing methods to detect and evaluate hallucination in large language models'
date: 2024-01-01
venue: 'King\'s College London'
paperurl: 'https://arxiv.org/abs/2404.12041'
citation: 'S Qi, L Gui, Y He, Z Yuan. (2024). &quot;A Survey of Automatic Hallucination Evaluation on Natural Language Generation.&quot; <i>arXiv preprint arXiv:2404.12041</i>.'
---

## Project Overview

This project focuses on developing comprehensive methods to detect and evaluate hallucination in large language models. Hallucination, where models generate false or misleading information, is a critical challenge in NLP that affects the reliability of AI systems.

## Key Contributions

- **Survey of Evaluation Methods**: Comprehensive review of existing hallucination evaluation techniques
- **Novel Detection Algorithms**: Development of new methods for identifying hallucinated content
- **Evaluation Framework**: Creation of standardized evaluation protocols for hallucination detection

## Technical Approach

The project employs multiple approaches:
- **Statistical Analysis**: Using statistical methods to identify inconsistencies
- **Semantic Verification**: Comparing generated content against source materials
- **Cross-Reference Checking**: Verifying claims against knowledge bases

## Results

- Published survey paper on arXiv with significant citations
- Developed evaluation toolkit for the research community
- Contributed to understanding of hallucination patterns in different model types

## Technologies Used

- Python, PyTorch, Transformers
- Natural Language Processing libraries
- Statistical analysis tools

## Project Status

**Ongoing** - Currently expanding to include more sophisticated detection methods and evaluation metrics. 